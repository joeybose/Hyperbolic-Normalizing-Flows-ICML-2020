\section{Related Work}
\xhdr{Hyperbolic Geometry in Machine Learning:}
The intersection of hyperbolic geometry and machine learning has recently risen to prominence \cite{dhingra2018embedding,tay2018hyperbolic,law2019lorentzian,khrulkov2019hyperbolic,ovinnikov2019poincar}. Early prior work proposed to embed data into the Poincar\'e ball model \cite{nickel2017poincare,chamberlain2017neural}.
The equivalent Lorentz model was later shown to have better numerical stability properties \cite{nickel2018learning}, and recent work has leveraged even more stable tiling approaches \cite{yu2019numerically}. 
%Recent works have also extended several conventional deep learning modules (e.g., dense neural network layers and GNN architectures) to hyperbolic space \cite{gulcehre2018hyperbolic,ganea2018hyperbolic, chami2019hyperbolic}.
%To provably bound the numerical error of hyperbolic embeddings in the Lorentz model \cite{yu2019numerically} propose another model of hyperbolic geometry using integer-based tiling. 
In addition, there exists a burgeoning literature of hyperbolic counterparts to conventional deep learning modules on Euclidean spaces, ---i.e. matrix multiplication, enabling the construction of hyperbolic neural networks (HNNs) \cite{gulcehre2018hyperbolic,ganea2018hyperbolic} with further extensions to graph data using hyperbolic GNN architectures \cite{liu2019graph,chami2019hyperbolic}.
Latent variable models on hyperbolic space have also been investigated in the context of VAEs, using  generalizations of the normal distribution \cite{nagano2019wrapped,mathieu2019continuous}.\cut{As an extension, \cite{skopek2019mixed} consider a more general case where the latent space of a VAE is a product of Riemannian manifolds with constant and learnable curvatures.} In contrast, our work learns a flexible approximate posterior using a novel normalizing flow designed to use the geometric structure of hyperbolic spaces.
In addition to work on hyperbolic VAEs, there are also several works that explore other non-Euclidean spaces (e.g., spherical VAEs) \cite{davidson2018hyperspherical,falorsi2019reparameterizing,grattarola2019adversarial}.

\xhdr{Learning Implicit Distributions}
In contrast with exact likelihood methods there is growing interest in learning implicit distributions for generative modelling. Popular approaches include density ratio estimation methods using a parametric classifiers such as GANS \cite{goodfellow2014generative}, and kernel based estimators \cite{shi2017kernel}. In the context of autoencoders learning implicit latent distribution can be seen as an adversarial game minimizing a specific divergence \cite{makhzani2015adversarial} or distance \cite{tolstikhin2017wasserstein}. Instead of adversarial formulations implicit distributions may also be learned directly by estimating the gradients of log density function using the Stein gradient estimator \cite{li2017gradient}. Finally, such gradient estimators can also be used to power variational inference with implicit posteriors enabling the use of posterior families with intractable densities  \cite{shi2018spectral}.

\xhdr{Normalizing Flows:} 
Normalizing flows (NFs)~\cite{rezende2015variational,dinh2016density} are a class of probabilistic models which use invertible transformations to map samples from a simple base distribution to samples from a more complex learned distribution.\cut{The new density can then be efficiently computed via the change of variable formula making normalizing flow powerful tools for variational inference and generative modeling.} While there are many classes of normalizing flows, see survey \cite{papamakarios2019normalizing,kobyzev2019normalizing}, our work largely follows flows designed with partial ordered dependency as found in affine coupling transformations \cite{dinh2016density}. 
Recently, normalizing flows have also been extended to Riemannian manifolds such as spherical spaces in \citet{gemici2016normalizing}. In parallel to this work, normalizing flows have also been extended to toriodal spaces \cite{rezende2020normalizing} and the data manifold \cite{brehmer2020flows}. Finally, relying on affine coupling and GNNs \cite{liu2019graph} develop graph normalizing flows (GNFs) for generating graphs, but unlike our approach GNFs do not benefit from the rich geometry of hyperbolic spaces which directly enable higher quality generated samples when the true data is tree-like.



