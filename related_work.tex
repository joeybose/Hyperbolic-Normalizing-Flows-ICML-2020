\section{Related Work}
\xhdr{Hyperbolic Geometry in Machine Learning:}
The intersection of hyperbolic geometry and machine learning has recently risen to prominence, \cite{dhingra2018embedding,tay2018hyperbolic,law2019lorentzian,khrulkov2019hyperbolic} as it provides an effective framework to continuously embed the discrete structured data ~\cite{krioukov2010hyperbolic}. Early prior work proposed to embed hierarchical data into the Poincar\'e disk model \cite{nickel2017poincare,chamberlain2017neural}.
The equivalent Lorentz model was later shown to have better numerical stability properties during optimization \cite{nickel2018learning}. 
To provably bound the numerical error of hyperbolic embeddings in the Lorentz model \cite{yu2019numerically} propose another model of hyperbolic geometry using integer-based tiling. 
Hyperbolic counterparts to conventional deep learning modules on Euclidean spaces, ---i.e. matrix multiplication, enabled the construction of hyperbolic neural networks (HNNs) \cite{gulcehre2018hyperbolic,ganea2018hyperbolic} and extensions to graph data \cite{liu2019graph,chami2019hyperbolic}.

Latent variable models on hyperbolic space have also been investigated in the context of VAEs with notable differences being in the generalizations of the normal distribution \cite{nagano2019wrapped,mathieu2019continuous}.\cut{As an extension, \cite{skopek2019mixed} consider a more general case where the latent space of a VAE is a product of Riemannian manifolds with constant and learnable curvatures.} In contrast, our work learns a flexible approximate posterior using a novel normalizing flows designed to use the geometry structure of hyperbolic spaces.

\xhdr{Normalizing Flows:} 
Normalizing flows (NFs)~\cite{rezende2015variational,dinh2016density} are a class of probabilistic models which use invertible transformations to map samples from a simple base distribution to samples from a more complex learned distribution.\cut{The new density can then be efficiently computed via the change of variable formula making normalizing flows powerful tools for variational inference and generative modeling.}While there many classes of normalizing flows, see survey \cite{papamakarios2019normalizing,kobyzev2019normalizing}, our work largely follows flows designed with partial ordered dependency as found in affine coupling transformations \cite{dinh2016density}. 

Recently, normalizing flows have been extended to Riemannian manifolds such as spherical spaces in \cite{gemici2016normalizing}.\cut{The idea is to (1) map a probability distribution on the Riemannian manifold to its homeomorphic Euclidean space, (2) apply the normalizing flow in this Euclidean space, and (3) map the distribution back to the manifold.} In a different line of work authors in~\cite{wang-wang-2019-riemannian} construct a planar flow  \cite{rezende2015variational} for VI on Riemannian manifolds parameterized by the inverse multiquadratics kernel function. Relying on affine coupling and GNNs \cite{liu2019graph} develop graph normalizing flows (GNFs) for generating graphs.



