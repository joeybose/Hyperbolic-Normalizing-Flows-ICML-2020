\section{Experiments}
We evaluate our $\mathcal{T}C$-flow and $\mathcal{W}\mathbb{H}C$-flow on three tasks, \ie, structured density estimation, graph reconstruction, and graph generation. 
Model details including hyperparameters and architectures can be found in Appendix \ref{model_arch_and_hyperparams}.

\xhdr{Setup and Baselines}
Throughout our experiments we rely on three main baselines, which we use to compare against normalizing flows constructed using $\mathcal{T}C$ and $\mathcal{W}\mathbb{H}C$ layers. In Euclidean space we use Gaussian latent variables and affine coupling flows \cite{dinh2016density}, denoted $\mathcal{N}$ and $\mathcal{N}C$, respectively. As we operate in the Lorentz model we further define Wrapped Gaussians latent variables, $\mathbb{H}$, in this space as an appropriate baseline \cite{nagano2019wrapped}. Since all model parameters are defined on tangent spaces, we use conventional optimizers like Adam \cite{kingma2014adam} for all models. 
Following previous work, we also consider the curvature $K$ as a learnable parameter with a warmup of $10$ epochs that progressively increasing curvature \cite{skopek2019mixed}.
\renjie{Increasing curvature or increase learning rate of curvature?}


\subsection{Structured Density Estimation}
We first consider structured density estimation in a canonical VAE setting, where we seek to learn rich approximated posteriors using normalizing flows and evaluate the marginal log-likelihood of test data. Following recent work on hyperbolic VAEs, we evaluate our normalizing flow on a branching diffusion process (BDP) \cite{mathieu2019continuous} and dynamically binarized MNIST \cite{skopek2019mixed}. 

Our results are shown in Tables \ref{bdp_table} and \ref{mnist_table}.
On both datasets we observe that our hyperbolic flows provide significant improvement over the baselines when a small latent dimensionality is used.
This result matches theoretical expectations and dovetails with previous work on graph embedding \cite{nickel2017poincare}, highlighting that the benefit of leveraging hyperbolic space is most prominent in small dimensions. 
For example, even a two-dimensional hyperbolic space can perfectly embed a tree with no distortion, which is impossible in Euclidean space.
However, as we increase the latent dimension, the Euclidean approaches can compensate for this intrinsic geometric limitation.  
\cut{
and we observe significant gains on both datasets when the latent dimension is small.\cut{In particular, $\mathcal{W}\mathbb{H}C$ performs the best with 2 dimensional latents with a relative improvement of \red{\%XX} over $\mathcal{N}C$ normalizing flows.} We reconcile this result by recalling that even in two dimensions hyperbolic spaces have more room to embed hierarchies. As we increase the latent dimension however we see that Euclidean based approaches outperform our proposed models which is line with similar observation in prior work \cite{nickel2017poincare}.}

\begin{table}[ht]
\label{bdp_table}
\begin{small}
\begin{center}
\begin{tabular}{lccccr}
    \toprule
    Model   &  BDP-2 & BDP-4 & BDP-6\\
    \midrule
    $\mathcal{N}$-VAE & $-55.4_{\pm 0.2}$  & $-55.2_{\pm 0.3}$& $-56.1_{\pm 0.2}$   \\
    $\mathbb{H}$-VAE & $-54.9_{\pm 0.3}$& $-55.4_{\pm 0.2}$ &  $-58.0_{\pm 0.2}$\\
    \cut{$\mathcal{P}$-VAE$^*$ & $-55.6_{\pm 0.2}$ & - &-  \\}
    \cut{$\mathbb{U}$-VAE & &  & $-55.8_{\pm 0.4}$  \\}
    $\mathcal{N}C$ & $-55.4_{\pm 0.4}$ & $ \textbf{-54.7}_{\pm 0.1}$ & $\textbf{-55.2}_{\pm 0.3}$  \\
    $\mathcal{T}C$& $-54.9_{\pm 0.1}$& $-55.6_{\pm 0.2}$& $-57.5_{\pm0.2}$\\
    $\mathcal{W}\mathbb{H}C$& $\textbf{-52.8}_{\pm 0.3}$ & $-55.2_{\pm 0.2}$& $-57.4_{\pm 0.3}$\\
    \bottomrule
\end{tabular}
\caption{Test Log Likelihood on Binary Diffusion Process versus latent dimension. All normalizing flows use 2-coupling layers.}
\end{center}
\vskip -0.1in
\end{small}
\end{table}

\begin{table}[ht]
\label{mnist_table}
\begin{small}
\begin{center}
\begin{tabular}{lcccc}
    \toprule
    Model   &  \shortstack{MNIST\\2} & \shortstack{MNIST\\4} & \shortstack{MNIST\\6}  \\
    \midrule
    $\mathcal{N}$-VAE &$-139.5_{\pm 1.0}$& $-115.6_{\pm0.2}$ & $-100.0_{\pm0.02}$ \\
    $\mathbb{H}$-VAE & $*$ & $-113.68_{\pm0.9}$& $-99.8_{\pm0.2}$ \\
    \cut{$\mathcal{P}$-VAE$^*$ & $-142.5_{\pm 0.4}$ & $-97.7_{\pm0.2}$&  \\}
    \cut{$\mathbb{U}$-VAE$^*$ & - & $-97.3_{\pm 0.2}$ &  \\}
    $\mathcal{N}C$ &  $-139.2_{\pm 0.4}$ & $-115.2_{\pm0.6}$& $\textbf{-98.7}_{0.3}$ \\
    $\mathcal{T}C$  & $*$& $ \textbf{-112.5}_{\pm0.2}$&$-99.3_{\pm0.2}$  \\
    $\mathcal{W}\mathbb{H}C$ & $\textbf{-136.5}_{\pm 2.1}$ & $-112.8_{\pm0.5}$ &$-99.4_{\pm0.2}$ \\
    \bottomrule
\end{tabular}
\caption{Test Log Likelihood on MNIST averaged over 5 runs verus latent dimension. * indicates numerically unstable settings.}
\end{center}
\vskip -0.1in
\vspace{-10pt}
\end{small}
\end{table}

\subsection{Graph Reconstruction}
We evaluate the practical utility of our hyperbolic flows by conducting experiments on the task of link prediction using a graph neural network (GNN) inference model. Given a simple graph $\mathcal{G}=(\V,A, X)$, defined by a set of nodes $\mathcal{V}$, an adjacency matrix $A \in \mathbb{Z}^{|\mathcal{V}| \times |\mathcal{V}|}$ and node feature matrix $X \in \mathbb{R}^{\mathcal{V} \times n}$, we learn a VGAE \cite{kipf2016variational} model whose inference network, $q_\phi$, defines a distribution over node embeddings $q_\phi(Z | A, X)$. To score the likelihood of an edge existing between pairs of nodes we use an inner product decoder: $p(A_{u,v}=1|z_u,z_v) = \sigma(z_u^Tz_v)$ (with dot products computed in $\mathcal{T}_{\textbf{o}}\mathbb{H}^n_K$ when necessary). Given these components, the inference GNNs are trained to minimize the variational lower bound on a training set of edges. 

We use two different disease datasets taken from \citep{chami2019hyperbolic} and \citep{mathieu2019continuous} for evaluation purposes. The first dataset Diseases-\RNum{1} is composed of a network of disorders and disease genes linked by known disorder–gene associations \cite{goh2007human}. In the second dataset Diseases-\RNum{2}, we build tree networks of an SIR disease spreading model \cite{anderson1992infectious}, where node features determine the susceptibility to the disease. In Table \ref{graph_embeddings_table} we report the AUC and average precision (AP) on the test set.
We observe consistent improvements when using hyperbolic $\mathcal{W}\mathbb{H}C$ flow. Similar to the structured density estimation setting, the performance gains of $\mathcal{W}\mathbb{H}C$ are best observed in low-dimensional latent spaces.


% In the first dataset, Diseases-\RNum{1}, we build tree networks of an SIR disease spreading model \cite{anderson1992infectious}, where node features determine the susceptibility to the disease. In Diseases-\RNum{2}, contains a network of disorders and disease genes linked by known disorder–gene associations \cite{goh2007human}. 

% \joey{Ariella can you just briefly comment on the empirical numbers here. Diesease-1 and 2 are LP and the other respectively.}

% \begin{table}[ht]
% \label{graph_embeddings_table}
% \begin{small}
% \begin{center}
% \begin{tabular}{lcccc}
%     \toprule
%     Model   & \shortstack{Dis-\RNum{1}\\AUC} & \shortstack{Dis-\RNum{1}\\AP}  & \shortstack{Dis-\RNum{2}\\AUC} & \shortstack{Dis-\RNum{1}\\AP}  \\
%     \midrule
%     $\mathcal{N}$-VAE & $0.89_{\pm 0.02}$ &
%     $0.91_{\pm 0.01}$ &
%     $0.92_{\pm 0.01}$ &
%     $0.91_{\pm 0.01}$ 
    
%     \\
%     $\mathbb{H}$-VAE & $0.90_{\pm 0.01}$ &
%     $0.91_{\pm 0.01}$ &
%     $0.92_{\pm 0.00}$ &
%     $0.91_{\pm 0.01}$ 
    
%     \\
%     $\mathcal{N}C$ & $0.91_{\pm 0.01}$ &
%     $0.92_{\pm 0.01}$ &
%     $0.95_{\pm 0.00}$ &
%     $0.93_{\pm 0.01}$ 
    
%     \\
%     $\mathcal{T}C$ & $\textbf{0.93}_{\pm 0.01}$&
%     $\textbf{0.93}_{\pm 0.00}$ &
%     $\textbf{0.96}_{\pm 0.01}$ &
%     $0.95_{\pm 0.01}$ 
    
%     \\
%     $\mathcal{W}\mathbb{H}C$ & $0.93_{\pm 0.01}$&
%     $0.92_{\pm 0.01}$ &
%     $\textbf{0.96}_{\pm 0.01}$ &
%     $\textbf{0.96}_{\pm 0.01}$ 
%     \\
%     \bottomrule
% \end{tabular}
% \end{center}
% \end{small}
% \caption{Test AUC and Test AP on Graph Embeddings where Dis-\RNum{1} has latent dimesion 6 and Dis-\RNum{2} has latent dimension 2.}
% \vskip -0.1in
% \end{table}

\begin{table}[ht]
\label{graph_embeddings_table}
\begin{small}
\begin{center}
\begin{tabular}{lcccc}
    \toprule
    Model   & \shortstack{Dis-\RNum{1}\\AUC} & \shortstack{Dis-\RNum{1}\\AP}  & \shortstack{Dis-\RNum{2}\\AUC} & \shortstack{Dis-\RNum{2}\\AP}  \\
    \midrule
    $\mathcal{N}$-VAE & $0.90_{\pm 0.01}$ &
    $0.92_{\pm 0.01}$ &
    $0.92_{\pm 0.01}$ &
    $0.91_{\pm 0.01}$
    
    \\
    $\mathbb{H}$-VAE & $0.91_{\pm 5\textnormal{e-3}}$ &
    $0.92_{\pm 5\textnormal{e-3}}$ &
    $0.92_{\pm 4\textnormal{e-3}}$ &
    $0.91_{\pm 0.01}$ 
    
    \\
    $\mathcal{N}C$ & $0.92_{\pm 0.01}$ &
    $0.93_{\pm 0.01}$ &
     $0.95_{\pm 4\textnormal{e-3}}$ &
    $0.93_{\pm 0.01}$ 
    
    \\
    $\mathcal{T}C$ & $\textbf{0.93}_{\pm 0.01}$ &
    $0.93_{\pm 0.01}$ &
   $\textbf{0.96}_{\pm 0.01}$ &
     $0.95_{\pm 0.01}$ 
    
    \\
    $\mathcal{W}\mathbb{H}C$ & $\textbf{0.93}_{\pm 0.01}$&
    $\textbf{0.94}_{\pm 0.01}$ &
    $\textbf{0.96}_{\pm 0.01}$ &
    $\textbf{0.96}_{\pm 0.01}$
    \\
    \bottomrule
\end{tabular}
\end{center}
\end{small}
\caption{Test AUC and Test AP on Graph Embeddings where Dis-\RNum{1} has latent dimesion 6 and Dis-\RNum{2} has latent dimension 2.}
\vskip -0.1in
\end{table}

\subsection{Graph Generation}

Finally, we explore utility of our hyperbolic flows for generating hierarchical structures. 
As a synthetic testbed, we construct datasets containing uniformly random trees as well as uniformly random lobster graphs \cite{golomb1996polyominoes}, where each graph contains between 20 and 100 nodes. 
We then train a generative graph model to learn the distribution of these datasets. 
We expect the hyperbolic flows to provide a significant benefit for generating valid random trees, as well as learning the distribution of lobster graphs (which are a special subset of trees). 

We follow the two stage training procedure outlined in Graph Normalizing Flows \cite{liu2019graph} in that we first train a deterministic autoencoder. Empirically, we find that defining edge probabilities using a distance based decoder consistently leads to better generation performance. Thus we define edge probabilities as, $p(A_{u,v}=1|z_u,z_v) = \sigma((-d_{\mathcal{G}}(u,v) - b)/\tau)$ where $b$ and $\tau$ are learned edge specific bias and temperature parameters. Starting from these fixed node embeddings we then train normalizing flows for density estimation. To model dependencies between node embeddings we use GRevNets \cite{liu2019graph} and an analagous version in hyperbolic space with graph attention \cite{velivckovic2017graph}. At inference time, we we first sample the number of nodes to generate from the empirical distribution of the dataset. We then independently sample node latents from our prior distribution and begin with a fully connected graph and then run our normalizing flow to give refined edge probabilities. 

\begin{figure*}
    \centering
    \includegraphics[width=\textwidth]{hyperbolic_graph_gen.pdf}
    \vspace{-5mm}
    \caption{Selected qualitative results on graph generation for lobster and random tree graph.}
    \label{fig:graph_generation_pic}
\end{figure*}

To evaluate the various approaches, we construct $100$ training graphs for each dataset to train our model and then evaluate the quality of the generated graphs. 
Figure \ref{fig:graph_generation_pic} shows representative samples learned by the various approaches.
We can see that the hyperbolic normalizing flows learn to generate tree-like graphs and also match the specific properties of the lobster graph distribution, whereas the Euclidean flow model tends to generate densely connected graphs with many cycles (or else disconnected graphs). 
To quantify these intuitions, Table \ref{tab:randtrees} contains statistics on how often the different models generate valid trees (denoted by ``accuracy''), as well as the average number of triangles and the average global clustering coefficients for the generated graphs. 
Since the target data is random trees, a perfect model would achieve 100\% accuracy, with no triangles, and a global clustering of 0 for all graphs. 
We see that the hyperbolic models generate valid trees more often, and they generate graphs with fewer triangles and lower clustering on average.
Finally, to evaluate how well the models match the specific properties of the lobster graphs, we follow \cite{liao2019efficient} and report the MMD distance between the generate graphs and a test set for various graph statistics (Figure \ref{fig:lobster_graph_gen}).
Again, we see that the hyperbolic approaches significantly outperform the Euclidean normalizing flow. 
\cut{
Our evaluation setup closely mirrors that of \cite{liao2019efficient} in that we construct $100$ training graphs with minimum and maximum number of nodes of $20$ and $100$ respectively, and calculate mean maximum discrepancy between distributions of graph statistics for generated and ground truth graphs. In table \ref{graph_gen_table} we present quantitative results for both datasets while Fig. \ref{fig:graph_generation_pic} shows selected generated samples.\cut{and find that both $\mathcal{T}C$ and $\mathcal{W}\mathbb{H}C$ achieve lower MMD scores on all metrics aside from clustering coefficient on Lobster graphs but achieves higher MMD scores on random trees.} As shown in Fig. \ref{fig:graph_generation_pic} generated samples using $\mathcal{N}C$ for the random trees dataset are simply shallow trees with only a handful of nodes which trivially have low MMD scores but do not respect the structure in the data. For lobster graphs, $\mathcal{N}C$ resembles closer to a fully connected graph and is unable to distinguish the hierarchy in the data. These empiricial observations are corroborated in the MMD metrics where we find that both $\mathcal{T}C$ and $\mathcal{W}\mathbb{H}C$ achieve lower MMD scorest. Critically, a significantly higher proportion of generated samples are actually lobster graphs, while none of the samples generated by $\mathcal{N}C$ respect this hierarchical structure.} \cut{Critically, samples generated by the different normalizing flow models demonstrate that flows on hyperbolic spaces are better able to capture the hierarchical structure of data with deeper hierarchies. Critically, a significantly higher proportion of generated samples are actually lobster graphs, while none of the samples generated by $\mathcal{N}C$ respect this hierarchical structure. Fig. \ref{fig:graph_generation_pic} shows selected s}
\cut{
\begin{table*}[]
\label{graph_gen_table}
\begin{scriptsize}
\begin{center}
\begin{tabular}{lllllllllll}
    \toprule
     & \multicolumn{5}{c}{Prufer} &  \multicolumn{5}{c}{Lobster} \\
    
    \midrule
    Model   & Deg. & Clus. & Orb. & Spec. & Acc. & Deg. & Clus. & Orb. & Spec. & Acc.\\
    \midrule
    $\mathcal{N}C$ &$\textbf{0.30}_{\pm0.12}$ & $\textbf{0.59}_{\pm0.40}$& $\textbf{0.57}_{\pm0.38}$& $\textbf{0.31}_{\pm0.15}$ & $\textbf{26.0}_{\pm25.0}$ & $0.47_{\pm3.0\textnormal{e-3}}$ & $1.02_{\pm7.6\textnormal{e-3}}$& $0.28_{\pm0.37}$& $0.50_{\pm0.01}$ & $0.00_{\pm0.0}$  \\
    $\mathcal{T}C$ & $0.31_{\pm0.09}$ & $1.01_{0.29}$& $0.89_{\pm0.11}$& $0.35_{\pm0.09}$ & $5.0_{\pm4.5}$ & $\textbf{0.39}_{\pm0.07}$ & $1.02_{0.62}$& $0.75_{\pm0.38}$& $0.40_{\pm0.11}$ & $\textbf{17.4}_{\pm33.7}$\\
    $\mathcal{W}\mathbb{H}C$ & $0.31_{\pm0.14}$ & $0.72_{\pm0.33}$&  $0.72_{\pm0.33}$& $0.32_{\pm0.14}$ & $16.3_{\pm20.9}$ & $0.41_{\pm0.07}$ & $\textbf{0.96}_{\pm0.43}$& $\textbf{0.47}_{\pm0.44}$& $\textbf{0.39}_{\pm0.13}$ & $11.8_{\pm23.1}$\\
    \bottomrule
\end{tabular}
\end{center}
\caption{Prufer and Lobster.}
\vskip -0.1in
\end{scriptsize}
\end{table*}
}

\begin{table}[]
\label{graph_gen_table}
%\begin{scriptsize}
\begin{center}
\begin{tabular}{llll}
    \toprule
    Model   & Accuracy & Avg. Clust. & Avg. Trans.\\
    \midrule
    $\mathcal{N}C$ & $56.6_{\pm 5.5}$ & $40.9_{\pm 42.7}$ & $0.34_{\pm0.10}$\\
    $\mathcal{T}C$ & $32.1_{\pm 1.9}$ & $98.3_{\pm 89.5}$ & $0.25_{\pm 0.12}$\\
    $\mathcal{W}\mathbb{H}C$ & $\textbf{62.1}_{\pm 10.9}$ & $\textbf{21.1}_{\pm 13.4}$ & $\textbf{0.13}_{\pm0.07}$\\
    \bottomrule
\end{tabular}
\end{center}
\caption{Graph statistics for generation on random trees.}
\label{tab:randtrees}
\vspace{-10pt}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{lobster_graph_gen.png}
    \vspace{-15pt}
    \caption{MMD scores for graph generation on Lobster graphs}
    \label{fig:lobster_graph_gen}
    \vspace{-15pt}
\end{figure}
%cccccccccc
