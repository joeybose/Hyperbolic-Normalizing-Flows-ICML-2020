\section{Related Work}
\xhdr{Hyperbolic Geometry in Machine Learning:}
The intersection of hyperbolic geometry and machine learning has recently risen to prominence \cite{dhingra2018embedding,tay2018hyperbolic,law2019lorentzian,khrulkov2019hyperbolic,ovinnikov2019poincar}. Early prior work proposed to embed data into the Poincar\'e ball model \cite{nickel2017poincare,chamberlain2017neural}.
The equivalent Lorentz model was later shown to have better numerical stability properties \cite{nickel2018learning}, and recent work has leveraged even more stable tiling approaches \cite{yu2019numerically}. 
Recent works have extended several conventional deep learning modules (e.g., dense neural network layers and GNN architectures) to hyperbolic space \cite{gulcehre2018hyperbolic,ganea2018hyperbolic, chami2019hyperbolic}.
%To provably bound the numerical error of hyperbolic embeddings in the Lorentz model \cite{yu2019numerically} propose another model of hyperbolic geometry using integer-based tiling. 
%Hyperbolic counterparts to conventional deep learning modules on Euclidean spaces, ---i.e. matrix multiplication, enabled the construction of hyperbolic neural networks (HNNs) \cite{gulcehre2018hyperbolic,ganea2018hyperbolic} and extensions to graph data \cite{liu2019graph,chami2019hyperbolic}.
Latent variable models on hyperbolic space have also been investigated in the context of VAEs, using  generalizations of the normal distribution \cite{nagano2019wrapped,mathieu2019continuous}.\cut{As an extension, \cite{skopek2019mixed} consider a more general case where the latent space of a VAE is a product of Riemannian manifolds with constant and learnable curvatures.} In contrast, our work learns a flexible approximate posterior using a novel normalizing flow designed to use the geometric structure of hyperbolic spaces.
In addition to work on hyperbolic VAEs, there are also several works that explore other non-Euclidean spaces (e.g., spherical VAEs) \cite{davidson2018hyperspherical,falorsi2019reparameterizing,grattarola2019adversarial}.

\xhdr{Normalizing Flows:} 
Normalizing flows (NFs)~\cite{rezende2015variational,dinh2016density} are a class of probabilistic models which use invertible transformations to map samples from a simple base distribution to samples from a more complex learned distribution.\cut{The new density can then be efficiently computed via the change of variable formula making normalizing flow powerful tools for variational inference and generative modeling.}While there are many classes of normalizing flows, see survey \cite{papamakarios2019normalizing,kobyzev2019normalizing}, our work largely follows flows designed with partial ordered dependency as found in affine coupling transformations \cite{dinh2016density}. 
Recently, normalizing flows have also been extended to Riemannian manifolds such as spherical spaces in \citet{gemici2016normalizing}.\cut{The the idea is to (1) map a probability distribution on the Riemannian manifold to its homeomorphic Euclidean space, (2) apply the normalizing flow in this Euclidean space, and (3) map the distribution back to the manifold.} In a different line of work authors in~\citet{wang-wang-2019-riemannian} construct a planar flow  \cite{rezende2015variational} on Riemannian manifolds parameterized by the inverse multiquadratics kernel function. %Relying on affine coupling and GNNs \cite{liu2019graph} develop graph normalizing flows (GNFs) for generating graphs.



